# Simple-Transformer

- Auther：Wanght
- Data：06/18/2024
- Email：819237474@qq.com

---

在这个Github库里，我们会将transformer的理论、训练、推理和消融实验进行较为全面的梳理，

追求能够通过这个项目达到对model的流利使用以及细节查验。

那么，让我们开始吧：

---

- #### 理论

首先，在开始之前，我希望你已经阅读了transformer最基础的两篇论文

如果没有，请放心，都为你准备好了，请阅读 [Attention is all you need](./ref/Transformer.pdf) 以及 [Transformer for image](./ref/ViT.pdf)

不出意外的话，你肯定对论文里的内容还存在些许疑惑，

没关系，我给你准备了较为详细的理论解析，请阅读 [这里](./theory/README.md)

----

- #### 训练&推理

在这里，我会手动搭建一个Transformer模型，并进行逐行解析清晰地进行理解。

以 ViT 识别 MNIST 为例。

##### Train:

```
python main.py
```

##### Eval:

```
python eval.py
```

更多我自己理论外的理解、更多细节以及逐行解析请看 [这里](./models/README.md)

----

<<<<<<< HEAD
- #### 消融实验

=======
- #### 实验

Transformer的训练主要是对那么几个参数进行调参：

内部参数：分块数patch size、Attention块层数depth、单层Attention数量head、感知机维度map_dim等

外部超参：学习率lr、batch size等

还有一些trick--lr策略、数据增强等

下面是一些调参实验：

ps: P:patch size | D:depth | H:head | M:mlp dim ---- e.g. P7D1H8M128 = 7分块1层深8头128维度

|   Scheme   | Epoch |  LR  |  Loss  |  Acc  |  Time   |
| :--------: | :---: | :--: | :----: | :---: | :-----: |
| P7D1H8M128 |   5   | 1e-4 | 0.1671 | 0.947 | 0:02:54 |
| P7D2H8M128 |   5   | 1e-4 | 0.1068 | 0.966 | 0:04:30 |
| P7D3H8M128 |   5   | 1e-4 | 0.0921 | 0.972 | 0:06:00 |
| P7D4H8M128 |   5   | 1e-4 | 0.0807 | 0.974 | 0:07:37 |
| P7D5H8M128 |   5   | 1e-4 | 0.0816 | 0.975 | 0:09:08 |
| P7D6H8M128 |   5   | 1e-4 | 0.0841 | 0.972 | 0:10:44 |

总结：Acc随Depth加深增加acc呈上升趋势，当然也不排除是浅层时训练时长不足导致低Acc，需要实验验证；

|   Scheme   | Epoch |  LR  |  Loss  |  Acc  |  Time   |
| :--------: | :---: | :--: | :----: | :---: | :-----: |
| P7D6H8M128 |  10   | 1e-2 | 0.8666 | 0.757 | 0:20:41 |
| P7D6H8M128 |  10   | 1e-3 | 0.0654 | 0.980 | 0:21:33 |
| P7D6H8M128 |  10   | 1e-4 | 0.0663 | 0.981 | 0:21:27 |
| P7D6H8M128 |  10   | 1e-5 | 0.1761 | 0.946 | 0:21:16 |
| P7D6H8M128 |  10   | 1e-6 | 1.3853 | 0.606 | 0:21:00 |
| P7D6H8M128 |  10   | 3e-2 | 2.3046 | 0.113 | 0:21:05 |
| P7D6H8M128 |  10   | 3e-3 | 0.0921 | 0.976 | 0:20:08 |
| P7D6H8M128 |  10   | 3e-4 | 0.0701 | 0.980 | 0:20:58 |
| P7D6H8M128 |  10   | 3e-5 | 0.0936 | 0.972 | 0:20:39 |
| P7D6H8M128 |  10   | 3e-6 | 0.4795 | 0.857 | 0:20:47 |

总结：学习率过大则极容易跨过极值点造成acc极速下降，奇怪的大的lr学习跨度速度反而变慢；学习率过大有步伐较小，则需要更多的epoch进行训练次啊能达到较好的效果。学习率越小所需迭代次数越大。

|   Scheme    | Epoch |  LR  |  Loss  |  Acc  |   Time   |
| :---------: | :---: | :--: | :----: | :---: | :------: |
| P1D2H8M128  |   1   | 1e-4 | 1.0893 | 0.600 | 1:56:20  |
| P1D1H8M128  |  10   | 1e-4 | 0.3662 | 0.884 | 10:14:36 |
| P2D1H8M128  |  10   | 1e-4 | 0.2329 | 0.924 | 0:37:27  |
| P4D1H8M128  |  10   | 1e-4 | 0.1699 | 0.947 | 0:09:03  |
| P7D1H8M128  |  10   | 1e-4 | 0.1214 | 0.963 | 0:05:40  |
| P14D1H8M128 |  10   | 1e-4 | 0.1019 | 0.969 | 0:04:36  |
| P28D1H8M128 |  10   | 1e-4 | 0.1206 | 0.963 | 0:04:20  |

从结果上来看，符合细粒度表征能力强的特性。

|      Scheme       | Epoch |  LR  |  Loss  |  Acc  |  Time   |
| :---------------: | :---: | :--: | :----: | :---: | :-----: |
|  P14D5H8M128-DM8  |  10   | 1e-4 | 0.1903 | 0.943 | 0:10:58 |
| P14D5H8M128-DM16  |  10   | 1e-4 | 0.1170 | 0.963 | 0:11:05 |
| P14D5H8M128-DM32  |  10   | 1e-4 | 0.0790 | 0.975 | 0:11:52 |
| P14D5H8M128-DM64  |  10   | 1e-4 | 0.0720 | 0.981 | 0:13:25 |
| P14D5H8M128-DM128 |  10   | 1e-4 | 0.0653 | 0.982 | 0:16:28 |
| P14D5H8M128-DM256 |  10   | 1e-4 | 0.0700 | 0.982 | 0:28:44 |
| P14D5H8M128-DM512 |  10   | 1e-4 | 0.0801 | 0.976 | 1:11:51 |

dim越大训练所需epoch越小

|   Scheme    | Epoch |  LR  |  Loss  |  Acc  |  Time   |
| :---------: | :---: | :--: | :----: | :---: | :-----: |
| P14D5H8M16  |  10   | 1e-4 | 0.0763 | 0.979 | 0:13:35 |
| P14D5H8M32  |  10   | 1e-4 | 0.0685 | 0.982 | 0:14:14 |
| P14D5H8M64  |  10   | 1e-4 | 0.0649 | 0.982 | 0:15:09 |
| P14D5H8M128 |  10   | 1e-4 | 0.0653 | 0.982 | 0:16:28 |
| P14D5H8M256 |  10   | 1e-4 | 0.0683 | 0.983 | 0:18:33 |
| P14D5H8M512 |  10   | 1e-4 | 0.0659 | 0.982 | 0:22:30 |

|    Scheme    | Epoch |  LR  |  Loss  |  Acc  |  Time   |
| :----------: | :---: | :--: | :----: | :---: | :-----: |
| P14D5H1M256  |  10   | 1e-4 | 0.0659 | 0.980 | 0:16:46 |
| P14D5H2M256  |  10   | 1e-4 | 0.0734 | 0.979 | 0:17:52 |
| P14D5H4M256  |  10   | 1e-4 | 0.0783 | 0.981 | 0:17:55 |
| P14D5H8M256  |  10   | 1e-4 | 0.0683 | 0.983 | 0:18:33 |
| P14D5H16M256 |  10   | 1e-4 | 0.0669 | 0.983 | 0:19:34 |

|      Scheme       | Epoch |  LR  |  Loss  |  Acc  |  Time   |
| :---------------: | :---: | :--: | :----: | :---: | :-----: |
|  P14D5H8M256-BS8  |  10   | 1e-4 | 0.0682 | 0.981 | 0:27:41 |
| P14D5H8M256-BS16  |  10   | 1e-4 | 0.0684 | 0.983 | 0:19:07 |
| P14D5H8M256-BS32  |  10   | 1e-4 | 0.0743 | 0.981 | 0:14:00 |
| P14D5H8M256-BS64  |  10   | 1e-4 | 0.0703 | 0.980 | 0:12:15 |
| P14D5H8M256-BS128 |  10   | 1e-4 | 0.0685 | 0.981 | 0:11:09 |
